{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81507FHUPRHm"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10Wf_UF0PRHx"
   },
   "source": [
    "# DoH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6454,
     "status": "ok",
     "timestamp": 1616573880125,
     "user": {
      "displayName": "Angelica Liguori",
      "photoUrl": "",
      "userId": "05017064839845812601"
     },
     "user_tz": -60
    },
    "id": "yoBBT_CpPRHy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score,auc, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, auc, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "device = torch.device('cuda' if (torch.cuda.is_available()) else 'cpu')\n",
    "print('[INFO] -> Using Device: ', device)\n",
    "\n",
    "from load_data import get_DoH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAUC_PRAUC(auc_list, pr_list, name, SPACE_AUC, SPACE_AUPRC):\n",
    "    print('AUC:')\n",
    "    print(auc_list)\n",
    "    print('**************')\n",
    "    print('PR AUC:')\n",
    "    print(pr_list)\n",
    "    print('**************')\n",
    "    \n",
    "    \n",
    "    AUC_Frame = pd.DataFrame(auc_list, columns = [name])\n",
    "    PR_AUC_Frame = pd.DataFrame(pr_list, columns = [name])\n",
    "    \n",
    "    AU_NAME = f'{name}.csv'\n",
    "    \n",
    "    AUC_Frame.to_csv(os.path.join(SPACE_AUC, AU_NAME), index=False)\n",
    "    PR_AUC_Frame.to_csv(os.path.join(SPACE_AUPRC, AU_NAME), index=False)\n",
    "    \n",
    "    AUC_Frame = list(AUC_Frame[name])\n",
    "\n",
    "    N = len(AUC_Frame)\n",
    "    mean_auc = np.mean(AUC_Frame)\n",
    "    std_auc = np.std(AUC_Frame)\n",
    "    std_error = std_auc / (np.sqrt(N))\n",
    "\n",
    "    ci = 1.96 * std_error\n",
    "    lower_bound = mean_auc - ci\n",
    "    upper_bound = mean_auc + ci\n",
    "    \n",
    "    print('AUC')\n",
    "    print(f'{mean_auc:.2f} +/- {ci:.2f}')\n",
    "    print(f'95% confidence level, average auc would be between {lower_bound:.2f} and {upper_bound:.2f}')\n",
    "    print('**************')\n",
    "    \n",
    "    PR_AUC_Frame = list(PR_AUC_Frame[name])\n",
    "\n",
    "    N = len(PR_AUC_Frame)\n",
    "    mean_auc = np.mean(PR_AUC_Frame)\n",
    "    std_auc = np.std(PR_AUC_Frame)\n",
    "    std_error = std_auc / (np.sqrt(N))\n",
    "\n",
    "    ci = 1.96 * std_error\n",
    "    lower_bound = mean_auc - ci\n",
    "    upper_bound = mean_auc + ci\n",
    "    \n",
    "    print('PR AUC')\n",
    "    print(f'{mean_auc:.2f} +/- {ci:.2f}')\n",
    "    print(f'95% confidence level, average auc would be between {lower_bound:.2f} and {upper_bound:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLoss(d_losses, g_losses, d_losses_val, bce_losses, rec_losses, real_scores, fake_scores, i, DATASET, show = False):\n",
    "    num_epochs = len(d_losses)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlim(0, num_epochs + 1)\n",
    "    plt.plot(range(1, num_epochs + 1), d_losses[:num_epochs], label='d loss')\n",
    "    plt.plot(range(1, num_epochs + 1), g_losses[:num_epochs], label='g loss')    \n",
    "\n",
    "    plt.plot(range(1, num_epochs + 1), d_losses_val[:num_epochs], '--', label='d loss val')\n",
    "\n",
    "    plt.legend(loc = 'lower right', bbox_to_anchor=(.8, 0.3, 0.5, 0.5))\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(f'./images/{i}_losses_{DATASET}.pdf')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlim(0, num_epochs + 1)\n",
    "    plt.plot(range(1, num_epochs + 1), g_losses[:num_epochs], label='g loss')    \n",
    "    plt.plot(range(1, num_epochs + 1), bce_losses[:num_epochs], label=r'$\\log(D(y))$')    \n",
    "    plt.plot(range(1, num_epochs + 1), rec_losses[:num_epochs], label=r'MSE')    \n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc = 'lower right', bbox_to_anchor=(.8, 0.3, 0.5, 0.5))\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(f'./images/{i}_gen_loss_{DATASET}.pdf')  \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlim(0, num_epochs + 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), fake_scores[:num_epochs], label='fake score')\n",
    "    plt.plot(range(1, num_epochs + 1), real_scores[:num_epochs], label='real score')    \n",
    "\n",
    "    plt.legend(loc = 'lower right', bbox_to_anchor=(.8, 0.3, 0.5, 0.5))\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(f'./images/{i}_score_{DATASET}.pdf') \n",
    "\n",
    "def generate_labels(size,pflip,lb,ub,step,decay=.9995,up=True): #decay=.99995\n",
    "    \n",
    "    if up:\n",
    "        lb = ub - (ub-lb)*((decay)**step)\n",
    "    else:\n",
    "        ub = lb + (ub-lb)*((decay)**step)\n",
    "    pflip = pflip*((decay)**step)\n",
    "    \n",
    "    y = np.random.uniform(lb, ub,size)   \n",
    "\n",
    "    sf = int(pflip*size)    \n",
    "    if sf > 0:\n",
    "        y[:sf] = 1- y[:sf]\n",
    "        np.random.shuffle(y)\n",
    "    \n",
    "    return torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nf_in = 121, nf_out = 32, z_dim = 16):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.nf_in = nf_in\n",
    "        self.nf_out = nf_out\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.nf_in, self.nf_out * 2), \n",
    "            nn.BatchNorm1d(self.nf_out * 2, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(self.nf_out * 2, self.nf_out),\n",
    "            nn.BatchNorm1d(self.nf_out, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.nf_out, self.nf_out * 2),\n",
    "            nn.BatchNorm1d(self.nf_out * 2, track_running_stats = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Linear(self.nf_out * 2, self.nf_in)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        self.init_weights()  \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        logits = self.decoder(enc)\n",
    "        \n",
    "        sampled_data = self.sigmoid(logits)\n",
    "        \n",
    "        return logits, sampled_data\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, nc = 121, nf_out = 16, nout = 128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.nc = nc\n",
    "        self.nf_out = nf_out\n",
    "        self.nout = nout\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # features extractor\n",
    "            nn.Linear(self.nc, self.nout),\n",
    "            nn.BatchNorm1d(self.nout, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(self.nout, self.nout * 2),\n",
    "            nn.BatchNorm1d(self.nout * 2, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(self.nout * 2, self.nout * 4),\n",
    "            nn.BatchNorm1d(self.nout * 4, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # classifier\n",
    "            nn.Linear(self.nout * 4, self.nout),\n",
    "            nn.BatchNorm1d(self.nout, track_running_stats = False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.nout, self.nf_out * 4),\n",
    "            nn.BatchNorm1d(self.nf_out * 4, track_running_stats = False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.nf_out * 4, self.nf_out * 2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.nf_out * 2, self.nf_out),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.nf_out, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.init_weights()  \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x.flatten()\n",
    "    \n",
    "\n",
    "class DiscriminatorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorLoss, self).__init__()\n",
    "        \n",
    "        self.criterion = nn.BCELoss() \n",
    "        \n",
    "    def forward(self, true_preds, fake_preds, step):\n",
    "        \n",
    "        bs = true_preds.size(0)\n",
    "        y_real = generate_labels(bs,0.3,0.7,1., step, up=True).to(device)\n",
    "    \n",
    "        D_real_loss = self.criterion(true_preds, y_real)\n",
    "    \n",
    "        y_fake = generate_labels(bs,0.3,0.,0.3, step, up=False).to(device)\n",
    "\n",
    "        D_fake_loss = self.criterion(fake_preds, y_fake)\n",
    "\n",
    "        return D_real_loss + D_fake_loss\n",
    "    \n",
    "    \n",
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        \n",
    "        self.criterion = nn.BCELoss() \n",
    "        self.mse = nn.MSELoss(reduction = 'mean')\n",
    "        self.cel = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    def forward(self, true_data, fake_preds, sampled_data, beta = 1, gamma = 1e-2):\n",
    "        \n",
    "        bs = fake_preds.size(0)\n",
    "                \n",
    "        y_fake = torch.ones(bs).to(device)\n",
    "        log_p_y = self.criterion(fake_preds, y_fake)\n",
    "        \n",
    "        rec = self.mse(true_data, sampled_data)\n",
    "        \n",
    "        return gamma*log_p_y + rec, log_p_y, rec\n",
    "    \n",
    "    \n",
    "    \n",
    "class AADNet(nn.Module):\n",
    "    def __init__(self, device, nc):\n",
    "        super(AADNet, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.nc = nc\n",
    "\n",
    "        self.D = Discriminator(nc = self.nc).to(self.device)\n",
    "        self.G = Generator(nf_in = self.nc).to(self.device)\n",
    "\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), lr=1e-5)\n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.d_loss = DiscriminatorLoss()\n",
    "        self.g_loss = GeneratorLoss()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "        \n",
    "    def D_step(self,true_data, step):\n",
    "        self.D.zero_grad()\n",
    "\n",
    "        logits, sampled_data = self.G(true_data)\n",
    "        true_pred = self.D(true_data)\n",
    "        fake_pred = self.D(sampled_data.detach())\n",
    "\n",
    "        d_loss_batch = self.d_loss(true_pred, fake_pred, step)\n",
    "        d_loss_batch.backward()\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        return d_loss_batch, true_pred, fake_pred\n",
    "\n",
    "    \n",
    "    def G_step(self,true_data, step):\n",
    "        self.G.zero_grad()\n",
    "        \n",
    "        logits, sampled_data = self.G(true_data)    \n",
    "        fake_pred = self.D(sampled_data)\n",
    "\n",
    "        gen_loss_batch, bce_loss, rec_loss = self.g_loss(true_data, fake_pred, sampled_data)\n",
    "        gen_loss_batch.backward()\n",
    "\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return gen_loss_batch, bce_loss, rec_loss\n",
    "\n",
    "            \n",
    "            \n",
    "    def evaluation(self, test_loader):\n",
    "        self.D.eval()\n",
    "        \n",
    "        d_l = []\n",
    "        ind = 0\n",
    "        \n",
    "        for batch, label in test_loader:\n",
    "            batch = batch.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred = self.D(batch)\n",
    "                \n",
    "                d_loss_batch = self.criterion(y_pred, label)\n",
    "                \n",
    "            d_l.append(d_loss_batch.item())\n",
    "        \n",
    "        return np.mean(d_l)\n",
    "    \n",
    "    def predict(self, test_loader):\n",
    "        self.D.eval()\n",
    "        i = 0\n",
    "        \n",
    "        for batch, label in test_loader:\n",
    "            batch = batch.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred = self.D(batch)\n",
    "                \n",
    "            if i == 0:\n",
    "                prediction = y_pred.cpu().round()\n",
    "                y_true = label.cpu()\n",
    "                yP = y_pred.cpu()\n",
    "            else:\n",
    "                prediction = torch.cat((prediction, y_pred.cpu().round()))\n",
    "                y_true = torch.cat((y_true, label.cpu()))\n",
    "                yP = torch.cat((yP, y_pred.cpu()))\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        return prediction, y_true, yP\n",
    "    \n",
    "    def plot_pr_curve(self, precision, recall):\n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision, marker='.')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.show()\n",
    "    \n",
    "    def pr_auc(self, y_test, y_pred):\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "        auc_score = auc(recall, precision)\n",
    "        print(f'PR AUC: {auc_score:.2f}')\n",
    "        self.plot_pr_curve(precision, recall)\n",
    "        return auc_score\n",
    "        \n",
    "        \n",
    "    def train(self, data_loader, test_loader, batch_size = 32, num_epochs = 10, step = 10, lowerbnd=5e-15, num_q_steps = 1, num_g_steps = 1):\n",
    "        \n",
    "        d_losses = np.zeros(num_epochs)\n",
    "        g_losses = np.zeros(num_epochs)\n",
    "        real_scores = np.zeros(num_epochs)\n",
    "        fake_scores = np.zeros(num_epochs)\n",
    "        rec_losses = np.zeros(num_epochs)\n",
    "        bce_losses = np.zeros(num_epochs)\n",
    "        \n",
    "        d_losses_val = np.zeros(num_epochs)\n",
    "        precision_abn = np.zeros(num_epochs)\n",
    "        recall_abn = np.zeros(num_epochs)\n",
    "        \n",
    "        \n",
    "        total_steps = (len(data_loader.dataset) // batch_size) #*num_epochs\n",
    "        print(\"[INFO] Starting training phase...\")\n",
    "        start = time()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            step_count = 0\n",
    "            for epoch in range(num_epochs):\n",
    "                self.D.train()\n",
    "                self.G.train()\n",
    "                i = 0\n",
    "                for batch in data_loader:\n",
    "                    \n",
    "                    step_count += 1\n",
    "                    batch = batch.to(self.device)\n",
    "\n",
    "                    ### Train autoencoder ###\n",
    "                    for _ in range(num_q_steps):\n",
    "                        d_loss, real_score, fake_score = self.D_step(batch,step_count)\n",
    "\n",
    "                    ### Train Generator ###\n",
    "                    for _ in range(num_g_steps):\n",
    "                        g_loss, bce_loss, rec_loss = self.G_step(batch,step_count)\n",
    "\n",
    "                    d_losses[epoch] = d_losses[epoch]*(i/(i+1.)) + d_loss.item()*(1./(i+1.))\n",
    "                    g_losses[epoch] = g_losses[epoch]*(i/(i+1.)) + g_loss.item()*(1./(i+1.))\n",
    "                    \n",
    "                    rec_losses[epoch] = rec_losses[epoch]*(i/(i+1.)) + rec_loss.item()*(1./(i+1.)) \n",
    "                    bce_losses[epoch] = bce_losses[epoch]*(i/(i+1.)) + bce_loss.item()*(1./(i+1.)) \n",
    "                    \n",
    "                    real_scores[epoch] = real_scores[epoch]*(i/(i+1.)) + real_score.mean().item()*(1./(i+1.))\n",
    "                    fake_scores[epoch] = fake_scores[epoch]*(i/(i+1.)) + fake_score.mean().item()*(1./(i+1.))\n",
    "\n",
    "                    \n",
    "                    i += 1\n",
    "                \n",
    "                dLossVal = self.evaluation(test_loader)\n",
    "                \n",
    "                d_losses_val[epoch] = dLossVal\n",
    "               \n",
    "                sys.stdout.write(\"\\r\" + 'Epoch [{:>3}/{}] | d_loss: {:.4f} | g_loss: {:.4f} ({:.2f}, {:.2f}) | D(x): {:.2f} | D(G(x)): {:.2f} | d_loss_val: {:.4f}'\n",
    "                              .format(epoch+1, num_epochs, d_losses[epoch], g_losses[epoch], bce_loss.item(), rec_losses[epoch], real_scores[epoch], fake_scores[epoch], d_losses_val[epoch]))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print('-' * 89)\n",
    "            print('[INFO] Exiting from training early')\n",
    "        print(f'\\n[INFO] Training phase... Elapsed time: {(time() - start):.0f} seconds\\n')\n",
    "        return d_losses[:epoch], g_losses[:epoch],rec_losses[:epoch], bce_losses[:epoch], real_scores[:epoch], fake_scores[:epoch], d_losses_val[:epoch]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1616573917983,
     "user": {
      "displayName": "Angelica Liguori",
      "photoUrl": "",
      "userId": "05017064839845812601"
     },
     "user_tz": -60
    },
    "id": "nvzpyul-PRID"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "WORK_SPACE = '../Dataset'\n",
    "DATASET_AREA = 'CIRA-CIC-DoHBrw-2020'\n",
    "\n",
    "SPACE_MODELS = './models'\n",
    "SPACE_AUC = './auc'\n",
    "SPACE_AUPRC = './auprc'\n",
    "\n",
    "for n in (SPACE_MODELS, SPACE_AUC, SPACE_AUPRC):\n",
    "    if not os.path.exists(n):\n",
    "        os.mkdir(n)\n",
    "\n",
    "FILE_3 = 'l2-benign.csv'\n",
    "FILE_4 = 'l2-malicious.csv'\n",
    "\n",
    "DATASET = 'DoH'\n",
    "\n",
    "PATH_B = os.path.join(WORK_SPACE, DATASET_AREA, FILE_3)\n",
    "PATH_M = os.path.join(WORK_SPACE, DATASET_AREA, FILE_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13436,
     "status": "ok",
     "timestamp": 1616573932393,
     "user": {
      "displayName": "Angelica Liguori",
      "photoUrl": "",
      "userId": "05017064839845812601"
     },
     "user_tz": -60
    },
    "id": "lV-_QYpwPRID",
    "outputId": "49afc328-2bf7-4070-ae8f-4cee2e471783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19807 normal records, 249836 anormal records\n",
      "We use 124918 anomalous records\n",
      "-----------------------------------------------------------------------------------------\n",
      "There are 15846 records in training set\n",
      "Training set is composed by 15846 normal records and 0 abnormal records\n",
      "-----------------------------------------------------------------------------------------\n",
      "There are 7236 records in validation set\n",
      "Validation set is composed by 990 normal records and 6246 abnormal records\n",
      "-----------------------------------------------------------------------------------------\n",
      "There are 21709 records in test set\n",
      "Test set is composed by 2971 normal records and 18738 abnormal records\n"
     ]
    }
   ],
   "source": [
    "dataset = get_DoH(PATH_B, PATH_M, seed, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean AUC and standard deviation after seven runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "auc_list = []\n",
    "num_epochs = 1500\n",
    "seed = 42\n",
    "pr_list = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    print(f'Iteration: {i+1}')\n",
    "    dataset = get_DoH(PATH_B, PATH_M, seed*(i+1), show=False)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=torch.FloatTensor(dataset['x_train']), \n",
    "                          batch_size = batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "    val_dataset = torch.utils.data.TensorDataset(torch.tensor(dataset['x_val']), \n",
    "                                              torch.tensor(dataset['y_val'].to_numpy()))\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.tensor(dataset['x_test']), \n",
    "                                              torch.tensor(dataset['y_test'].to_numpy()))\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    trainer = AADNet(device, dataset['x_train'].shape[1])\n",
    "    \n",
    "    d_losses, g_losses, rec_losses, bce_losses, real_scores, fake_scores, d_losses_val = trainer.train(train_loader, val_loader, batch_size = batch_size, num_epochs = num_epochs)\n",
    "    \n",
    "    # Save models\n",
    "    NAME_D = f'D_GCN_NOVG_{DATASET}_{i}.ckpt'\n",
    "    NAME_G = f'G_GCN__NOVG_{DATASET}_{i}.ckpt'\n",
    "    \n",
    "    torch.save(trainer.D.state_dict(), os.path.join(SPACE_MODELS, NAME_D))\n",
    "    torch.save(trainer.G.state_dict(), os.path.join(SPACE_MODELS, NAME_G))\n",
    "    \n",
    "    plotLoss(d_losses, g_losses, d_losses_val, bce_losses, rec_losses, real_scores, fake_scores, i, DATASET, show = True)\n",
    "    \n",
    "    prediction, y_true, yP = trainer.predict(test_loader)\n",
    "    _auc = roc_auc_score(1-y_true, 1-yP)\n",
    "    pr = trainer.pr_auc(1-y_true, 1-yP)\n",
    "    print(f'AUC: {_auc:.2f}')\n",
    "    \n",
    "    auc_list.append(_auc)\n",
    "    pr_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanAUC_PRAUC(auc_list, pr_list, 'GCN_NOVG_DoH', SPACE_AUC, SPACE_AUPRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PwEZM6YOG9R"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia di modelloDeep.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
