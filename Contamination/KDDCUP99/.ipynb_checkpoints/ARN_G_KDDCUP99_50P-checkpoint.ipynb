{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score,auc, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, auc, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "device = torch.device('cuda' if (torch.cuda.is_available()) else 'cpu')\n",
    "print('[INFO] -> Using Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_text_dummy(df, name):\n",
    "    \n",
    "    names = []\n",
    "    dummies = pd.get_dummies(df.loc[:,name])\n",
    "    i = 0\n",
    "    \n",
    "    tmpL = []\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df.loc[:, dummy_name] = dummies[x]\n",
    "        names.append(dummy_name)\n",
    "        _x = [i, x]\n",
    "        tmpL.append(_x)\n",
    "        i += 1\n",
    "    \n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "    return names, tmpL\n",
    "\n",
    "\n",
    "def _to_xy(df, target):\n",
    "    \"\"\"Converts a Pandas dataframe to the x,y inputs\"\"\"\n",
    "    y = df[target]\n",
    "    x = df.drop(columns=target)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_KDDCUP99(PATH, seed, percAnomalies = 0.01, scale = True, show = False):\n",
    "    \n",
    "    columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n",
    "        'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
    "        'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
    "        'num_access_files', 'num_outbound_cmds', 'is_hot_login',\n",
    "        'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
    "        'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
    "        'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "        'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "        'dst_host_srv_serror_rate', 'dst_host_rerror_rate','dst_host_srv_rerror_rate', 'label']\n",
    "    \n",
    "    df = pd.read_csv(PATH, header=None, names=columns)\n",
    "    discreteCol = ['protocol_type', 'service', 'flag', 'land', 'logged_in', 'is_hot_login', 'is_guest_login']\n",
    "    \n",
    "    names = []\n",
    "    oneHot = dict()\n",
    "    for name in discreteCol:\n",
    "        n, t = _encode_text_dummy(df, name)\n",
    "        names.extend(n)\n",
    "        oneHot[name] = t\n",
    "\n",
    "    labels = df['label'].copy()\n",
    "    labels[labels != 'normal.'] = 0 # anomalous\n",
    "    labels[labels == 'normal.'] = 1 # normal\n",
    "\n",
    "    df['label'] = labels\n",
    "    normal = df[df['label'] == 1] \n",
    "    abnormal = df[df['label'] == 0]\n",
    "    \n",
    "    normal = shuffle(normal, random_state = seed)\n",
    "    abnormal = shuffle(abnormal, random_state = seed)\n",
    "\n",
    "    abnormal_1 = abnormal[:int(len(abnormal)*.5)+1]\n",
    "    abnormal_2 = abnormal[int(len(abnormal)*.5)+1:]\n",
    "\n",
    "    train_size = int(len(normal)*.8)\n",
    "    val_size = int(len(normal)*.05)+1\n",
    "\n",
    "    train_normal = normal[:train_size]\n",
    "    val_normal = normal[train_size: train_size+val_size]\n",
    "    test_normal = normal[train_size+val_size: ]\n",
    "    \n",
    "    train_abnormal = abnormal_2[:int(len(train_normal)*percAnomalies)]\n",
    "\n",
    "    val_size = int(len(abnormal_1)*.05)+1\n",
    "    test_size = int(len(abnormal_1)*.15)+1\n",
    "    val_abnormal = abnormal_1[:val_size]\n",
    "    test_abnormal = abnormal_1[val_size:val_size+test_size]\n",
    "\n",
    "    train_set = pd.concat((train_normal, train_abnormal))\n",
    "    val_set = pd.concat((val_normal, val_abnormal))\n",
    "    test_set = pd.concat((test_normal, test_abnormal))\n",
    "    \n",
    "    train_set['label'].replace({0:1}, inplace = True)\n",
    "    \n",
    "    x_train, y_train = _to_xy(train_set, target='label')\n",
    "    x_val, y_val = _to_xy(val_set, target='label')\n",
    "    x_test, y_test = _to_xy(test_set, target='label')\n",
    "    \n",
    "    \n",
    "    if show:\n",
    "        print('{} normal records, {} anormal records'.format(len(normal), len(abnormal)))\n",
    "        print(f'We use {len(abnormal_1)} anomalous records')\n",
    "        print('-' * 89)\n",
    "        print(f'There are {len(x_train)} records in training set')\n",
    "        print(f'Training set is composed by {len(x_train[y_train == 1])} normal records and {len(x_train[y_train == 0])} abnormal records')\n",
    "        print(f'There are {len(train_abnormal)} records anomalies labeled as normal')\n",
    "        print('-' * 89)\n",
    "        print(f'There are {len(x_val)} records in validation set')\n",
    "        print(f'Validation set is composed by {len(x_val[y_val == 1])} normal records and {len(x_val[y_val == 0])} abnormal records')\n",
    "        print('-' * 89)\n",
    "        print(f'There are {len(x_test)} records in test set')\n",
    "        print(f'Test set is composed by {len(x_test[y_test == 1])} normal records and {len(x_test[y_test == 0])} abnormal records')\n",
    "\n",
    "    selected_columns = dict()\n",
    "    \n",
    "    for name in discreteCol:\n",
    "        cols = [col for col in names if name in col]\n",
    "        tmp = []\n",
    "        \n",
    "        for c in cols:\n",
    "            tmp.append(x_train.columns.get_loc(c))\n",
    "\n",
    "        selected_columns[name] = tmp\n",
    "    \n",
    "    x_train = x_train.to_numpy()\n",
    "    x_val = x_val.to_numpy()\n",
    "    x_test = x_test.to_numpy()\n",
    "    \n",
    "    index = np.arange(0, len(columns)-len(discreteCol)-1)\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(x_train[:, index])\n",
    "        x_train[:, index] = scaler.transform(x_train[:, index])\n",
    "        x_val[:, index] = scaler.transform(x_val[:, index])\n",
    "        x_test[:, index] = scaler.transform(x_test[:, index])\n",
    "        \n",
    "        \n",
    "    dataset = {}\n",
    "    dataset['x_train'] = x_train.astype(np.float32)\n",
    "    dataset['y_train'] = y_train.astype(np.float32)\n",
    "    \n",
    "    dataset['x_val'] = x_val.astype(np.float32)\n",
    "    dataset['y_val'] = y_val.astype(np.float32)\n",
    "    \n",
    "    dataset['x_test'] = x_test.astype(np.float32)\n",
    "    dataset['y_test'] = y_test.astype(np.float32)\n",
    "    \n",
    "    dataset['selectedColumns'] = selected_columns\n",
    "    dataset['discreteCol'] = discreteCol\n",
    "    dataset['oneHot'] = oneHot\n",
    "    dataset['index'] = index\n",
    "    dataset['scaler'] = scaler\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "WORK_SPACE = '../Dataset'\n",
    "FILE = 'kddcup.data_10_percent_corrected'\n",
    "\n",
    "DATASET = 'KDDCUP99'\n",
    "\n",
    "PATH = os.path.join(WORK_SPACE, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_KDDCUP99(PATH, seed, percAnomalies = 0.5, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(size,pflip,lb,ub,step,decay=.9995,up=True): #decay=.99995\n",
    "    \n",
    "    if up:\n",
    "        lb = ub - (ub-lb)*((decay)**step)\n",
    "    else:\n",
    "        ub = lb + (ub-lb)*((decay)**step)\n",
    "    pflip = pflip*((decay)**step)\n",
    "    \n",
    "    y = np.random.uniform(lb, ub,size)   \n",
    "\n",
    "    sf = int(pflip*size)    \n",
    "    if sf > 0:\n",
    "        y[:sf] = 1- y[:sf]\n",
    "        np.random.shuffle(y)\n",
    "    \n",
    "    return torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape).to(device)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = torch.log_softmax(logits, dim=-1) + sample_gumbel(logits.size())\n",
    "    return torch.softmax(y / temperature, dim=-1).to(device)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1e-5):\n",
    "    \"\"\"\n",
    "    input: [*, n_class]\n",
    "    return: [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    return (y_hard - y).detach() + y\n",
    "\n",
    "\n",
    "def gumbel_sigmoid_sample(logits, temperature):\n",
    "    # See https://davidstutz.de/categorical-variational-auto-encoders-and-the-gumbel-trick/\n",
    "    u = torch.rand_like(logits)\n",
    "    # we exploit the fact log(sigma(x)) - log(1-sigma(x)) = x\n",
    "    y = logits + torch.log(u) - torch.log(1 - u)\n",
    "    \n",
    "    return torch.sigmoid(y / temperature)\n",
    "\n",
    "\n",
    "def gumbel_sigmoid(logits, temperature=1e-5):\n",
    "    \"\"\"\n",
    "    input: [*]\n",
    "    return: [*] a binary response\n",
    "    \"\"\"\n",
    "    y = gumbel_sigmoid_sample(logits, temperature)\n",
    "    y_hard = (y > .5).float()\n",
    "    return (y_hard - y).detach() + y\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nf_in = 121, nf_out = 32, z_dim = 16):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.nf_in = nf_in\n",
    "        self.nf_out = nf_out\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.nf_in, self.nf_out * 2), \n",
    "            nn.BatchNorm1d(self.nf_out * 2, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(self.nf_out * 2, self.nf_out),\n",
    "            nn.BatchNorm1d(self.nf_out, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.nf_out, self.nf_out * 2),\n",
    "            nn.BatchNorm1d(self.nf_out * 2, track_running_stats = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(self.nf_out * 2, self.nf_in)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(self.nf_out, self.nf_out)\n",
    "        self.fc21 = nn.Linear(self.nf_out, self.z_dim)\n",
    "        self.fc22 = nn.Linear(self.nf_out, self.z_dim)\n",
    "\n",
    "        self.fc3 = nn.Linear(self.z_dim, self.nf_out)\n",
    "        self.fc4 = nn.Linear(self.nf_out, self.nf_out)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        self.init_weights()  \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc21(h), self.fc22(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        conv = self.encoder(x)\n",
    "        h = self.fc1(conv)\n",
    "        \n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.relu(self.fc3(z))\n",
    "        deconv_input = self.fc4(h)\n",
    "        \n",
    "        return self.decoder(deconv_input)\n",
    "    \n",
    "    def gumbel(self, logits, t):\n",
    "        return gumbel_softmax(logits, t)\n",
    "\n",
    "    def forward(self, x, text_l, selected_columnsTrain, index, t=1):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        logits = self.decode(z)\n",
    "        \n",
    "        sampled_data = logits.clone()\n",
    "        sampled_data[:, index] = self.sigmoid(logits[:, index])\n",
    "        for name in text_l:\n",
    "            sampled_data[:, selected_columnsTrain[name]] = self.gumbel(logits[:, selected_columnsTrain[name]], t)\n",
    "        \n",
    "        return logits, mu, logvar, sampled_data\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, nc = 121, nf_out = 16, nout = 128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.nc = nc\n",
    "        self.nf_out = nf_out\n",
    "        self.nout = nout\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # features extractor\n",
    "            nn.Linear(self.nc, self.nout),\n",
    "            nn.BatchNorm1d(self.nout, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(self.nout, self.nout * 2),\n",
    "            nn.BatchNorm1d(self.nout * 2, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(self.nout * 2, self.nout * 4),\n",
    "            nn.BatchNorm1d(self.nout * 4, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # classifier\n",
    "            nn.Linear(self.nout * 4, self.nout),\n",
    "            nn.BatchNorm1d(self.nout, track_running_stats = False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.nout, self.nf_out * 4),\n",
    "            nn.BatchNorm1d(self.nf_out * 4, track_running_stats = False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.nf_out * 4, self.nf_out * 2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.nf_out * 2, self.nf_out),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.nf_out, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.init_weights()  \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x.flatten()\n",
    "    \n",
    "\n",
    "class DiscriminatorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorLoss, self).__init__()\n",
    "        \n",
    "        self.criterion = nn.BCELoss() \n",
    "        \n",
    "    def forward(self, true_preds, fake_preds, step):\n",
    "        \n",
    "        bs = true_preds.size(0)\n",
    "        y_real = generate_labels(bs,0.3,0.7,1., step, up=True).to(device)\n",
    "    \n",
    "        D_real_loss = self.criterion(true_preds, y_real)\n",
    "    \n",
    "        y_fake = generate_labels(bs,0.3,0.,0.3, step, up=False).to(device)\n",
    "\n",
    "        D_fake_loss = self.criterion(fake_preds, y_fake)\n",
    "\n",
    "        return D_real_loss + D_fake_loss\n",
    "    \n",
    "    \n",
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self, text_l, selected_columnsTrain, index):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        \n",
    "        self.text_l = text_l\n",
    "        self.selected_columnsTrain = selected_columnsTrain\n",
    "        self.index = index\n",
    "        \n",
    "        self.criterion = nn.BCELoss() \n",
    "        self.mse = nn.MSELoss(reduction = 'mean')\n",
    "        self.cel = nn.CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "    def KLD(self,z_mean, z_logvar):\n",
    "        return torch.mean(0.5 * (-0.5 * z_logvar + torch.exp(0.5 * z_logvar) + z_mean ** 2))\n",
    "    \n",
    "    \n",
    "    def reconstruction(self, true_data, sampled_data):\n",
    "        g1 = self.mse(true_data[:, self.index], sampled_data[:, self.index])\n",
    "        g2 = 0\n",
    "        for name in self.text_l:\n",
    "            y = true_data[:, self.selected_columnsTrain[name]]\n",
    "            y_p = sampled_data[:, self.selected_columnsTrain[name]]\n",
    "            g2 += self.mse(y_p, y)\n",
    "        g2 /= len(self.text_l)\n",
    "        return g1 + 0.5 * g2\n",
    "\n",
    "    def forward(self, true_data, fake_preds, sampled_data, z_mean, z_logvar, beta = 1, gamma = 1e-2):\n",
    "        \n",
    "        bs = fake_preds.size(0)\n",
    "                \n",
    "        y_fake = torch.ones(bs).to(device)\n",
    "        log_p_y = self.criterion(fake_preds, y_fake)\n",
    "        \n",
    "        rec = self.reconstruction(true_data, sampled_data)\n",
    "        \n",
    "        kld = self.KLD(z_mean, z_logvar)\n",
    "        \n",
    "        return gamma*log_p_y + rec + beta*kld, log_p_y, rec, kld\n",
    "    \n",
    "    \n",
    "    \n",
    "class AADNet(nn.Module):\n",
    "    def __init__(self, device, selected_columns, discreteCol, index, nc):\n",
    "        super(AADNet, self).__init__()\n",
    "        self.device = device\n",
    "        self.selected_columns = selected_columns\n",
    "        self.discreteCol = discreteCol\n",
    "        self.index = index\n",
    "        self.nc = nc\n",
    "\n",
    "        self.D = Discriminator(nc = self.nc).to(self.device)\n",
    "        self.G = Generator(nf_in = self.nc).to(self.device)\n",
    "\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), lr=1e-5)\n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.d_loss = DiscriminatorLoss()\n",
    "        self.g_loss = GeneratorLoss(self.discreteCol, self.selected_columns, self.index)\n",
    "        \n",
    "        self.temperature = 1\n",
    "        self.anneal = 0.9995\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "        \n",
    "    def D_step(self,true_data, step):\n",
    "        self.D.zero_grad()\n",
    "\n",
    "        logits, _, _, sampled_data = self.G(true_data, self.discreteCol, self.selected_columns, self.index, self.temperature)\n",
    "        true_pred = self.D(true_data)\n",
    "        fake_pred = self.D(sampled_data.detach())\n",
    "\n",
    "        d_loss_batch = self.d_loss(true_pred, fake_pred, step)\n",
    "        d_loss_batch.backward()\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        return d_loss_batch, true_pred, fake_pred\n",
    "\n",
    "    \n",
    "    def G_step(self,true_data, step):\n",
    "        self.G.zero_grad()\n",
    "        \n",
    "        logits, z_mean, z_logvar, sampled_data = self.G(true_data, self.discreteCol, self.selected_columns, \n",
    "                                                        self.index, self.temperature)    \n",
    "        fake_pred = self.D(sampled_data)\n",
    "\n",
    "        gen_loss_batch, bce_loss, rec_loss, kl = self.g_loss(true_data, fake_pred, sampled_data, z_mean, z_logvar, self.temperature)\n",
    "        gen_loss_batch.backward()\n",
    "\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return gen_loss_batch, bce_loss, rec_loss, kl\n",
    "\n",
    "    \n",
    "    def anneal_temp(self, lowerbound=1e-5):\n",
    "        if self.temperature > lowerbound:\n",
    "            self.temperature = self.temperature*self.anneal\n",
    "            \n",
    "            \n",
    "    def evaluation(self, test_loader):\n",
    "        self.D.eval()\n",
    "        \n",
    "        d_l = []\n",
    "        ind = 0\n",
    "        \n",
    "        for batch, label in test_loader:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred = self.D(batch)\n",
    "                \n",
    "                d_loss_batch = self.criterion(y_pred, label)\n",
    "                \n",
    "            d_l.append(d_loss_batch.item())\n",
    "        \n",
    "        return np.mean(d_l)\n",
    "    \n",
    "    def predict(self, test_loader):\n",
    "        self.D.eval()\n",
    "        i = 0\n",
    "        \n",
    "        for batch, label in test_loader:\n",
    "            batch = batch.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred = self.D(batch)\n",
    "                \n",
    "            if i == 0:\n",
    "                prediction = y_pred.cpu().round()\n",
    "                y_true = label.cpu()\n",
    "                yP = y_pred.cpu()\n",
    "            else:\n",
    "                prediction = torch.cat((prediction, y_pred.cpu().round()))\n",
    "                y_true = torch.cat((y_true, label.cpu()))\n",
    "                yP = torch.cat((yP, y_pred.cpu()))\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        return prediction, y_true, yP\n",
    "        \n",
    "        \n",
    "    def train(self, data_loader, test_loader, batch_size = 32, num_epochs = 10, step = 10, lowerbnd=5e-15, num_q_steps = 1, num_g_steps = 1):\n",
    "        \n",
    "        d_losses = np.zeros(num_epochs)\n",
    "        g_losses = np.zeros(num_epochs)\n",
    "        real_scores = np.zeros(num_epochs)\n",
    "        fake_scores = np.zeros(num_epochs)\n",
    "        rec_losses = np.zeros(num_epochs)\n",
    "        bce_losses = np.zeros(num_epochs)\n",
    "        kldes = np.zeros(num_epochs)\n",
    "        \n",
    "        d_losses_val = np.zeros(num_epochs)\n",
    "        precision_abn = np.zeros(num_epochs)\n",
    "        recall_abn = np.zeros(num_epochs)\n",
    "        \n",
    "        \n",
    "        self.temperature = 1.\n",
    "        \n",
    "        total_steps = (len(data_loader.dataset) // batch_size) #*num_epochs\n",
    "        print(\"[INFO] Starting training phase...\")\n",
    "        start = time()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            step_count = 0\n",
    "            for epoch in range(num_epochs):\n",
    "                self.D.train()\n",
    "                self.G.train()\n",
    "                i = 0\n",
    "                for batch in data_loader:\n",
    "                    \n",
    "                    step_count += 1\n",
    "                    batch = batch.to(self.device)\n",
    "\n",
    "                    ### Train autoencoder ###\n",
    "                    for _ in range(num_q_steps):\n",
    "                        d_loss, real_score, fake_score = self.D_step(batch,step_count)\n",
    "\n",
    "                    ### Train Generator ###\n",
    "                    for _ in range(num_g_steps):\n",
    "                        g_loss, bce_loss, rec_loss, kl = self.G_step(batch,step_count)\n",
    "\n",
    "                    d_losses[epoch] = d_losses[epoch]*(i/(i+1.)) + d_loss.item()*(1./(i+1.))\n",
    "                    g_losses[epoch] = g_losses[epoch]*(i/(i+1.)) + g_loss.item()*(1./(i+1.))\n",
    "                    \n",
    "                    rec_losses[epoch] = rec_losses[epoch]*(i/(i+1.)) + rec_loss.item()*(1./(i+1.)) \n",
    "                    bce_losses[epoch] = bce_losses[epoch]*(i/(i+1.)) + bce_loss.item()*(1./(i+1.)) \n",
    "                    kldes[epoch] = kldes[epoch]*(i/(i+1.)) + kl.item()*(1./(i+1.))\n",
    "                    \n",
    "                    real_scores[epoch] = real_scores[epoch]*(i/(i+1.)) + real_score.mean().item()*(1./(i+1.))\n",
    "                    fake_scores[epoch] = fake_scores[epoch]*(i/(i+1.)) + fake_score.mean().item()*(1./(i+1.))\n",
    "\n",
    "                    # Anneal the temperature along with training steps\n",
    "                    self.anneal_temp(lowerbnd)\n",
    "                    \n",
    "                    i += 1\n",
    "                \n",
    "                dLossVal = self.evaluation(test_loader)\n",
    "                \n",
    "                d_losses_val[epoch] = dLossVal\n",
    "               \n",
    "                sys.stdout.write(\"\\r\" + 'Epoch [{:>3}/{}] | d_loss: {:.4f} | g_loss: {:.4f} ({:.2f}, {:.2f}, {:.2f}) | D(x): {:.2f} | D(G(x)): {:.2f} | d_loss_val: {:.4f}'\n",
    "                              .format(epoch+1, num_epochs, d_losses[epoch], g_losses[epoch], bce_loss.item(), rec_losses[epoch], kldes[epoch], real_scores[epoch], fake_scores[epoch], d_losses_val[epoch]))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print('-' * 89)\n",
    "            print('[INFO] Exiting from training early')\n",
    "        print(f'\\n[INFO] Training phase... Elapsed time: {(time() - start):.0f} seconds\\n')\n",
    "        return d_losses[:epoch], g_losses[:epoch],rec_losses[:epoch], bce_losses[:epoch], kldes[:epoch], real_scores[:epoch], fake_scores[:epoch], d_losses_val[:epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLoss(d_losses, g_losses, d_losses_val, bce_losses, rec_losses, kldes, real_scores, fake_scores, i, DATASET, show = False):\n",
    "    num_epochs = len(d_losses)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlim(0, num_epochs + 1)\n",
    "    plt.plot(range(1, num_epochs + 1), d_losses[:num_epochs], label='d loss')\n",
    "    plt.plot(range(1, num_epochs + 1), g_losses[:num_epochs], label='g loss')    \n",
    "\n",
    "    plt.plot(range(1, num_epochs + 1), d_losses_val[:num_epochs], '--', label='d loss val')\n",
    "\n",
    "    plt.legend(loc = 'lower right', bbox_to_anchor=(.8, 0.3, 0.5, 0.5))\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(f'./images/{i}_losses_{DATASET}.pdf')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlim(0, num_epochs + 1)\n",
    "    plt.plot(range(1, num_epochs + 1), g_losses[:num_epochs], label='g loss')    \n",
    "    plt.plot(range(1, num_epochs + 1), bce_losses[:num_epochs], label=r'$\\log(D(y))$')    \n",
    "    plt.plot(range(1, num_epochs + 1), rec_losses[:num_epochs], label=r'MSE')    \n",
    "    plt.plot(range(1, num_epochs + 1), kldes[:num_epochs], label='KLD') \n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc = 'lower right', bbox_to_anchor=(.8, 0.3, 0.5, 0.5))\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(f'./images/{i}_gen_loss_{DATASET}.pdf')  \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlim(0, num_epochs + 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), fake_scores[:num_epochs], label='fake score')\n",
    "    plt.plot(range(1, num_epochs + 1), real_scores[:num_epochs], label='real score')    \n",
    "\n",
    "    plt.legend(loc = 'lower right', bbox_to_anchor=(.8, 0.3, 0.5, 0.5))\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(f'./images/{i}_score_{DATASET}.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "auc_list = []\n",
    "num_epochs = 400\n",
    "seed = 42\n",
    "\n",
    "for i in range(n_runs):\n",
    "    print(f'Iteration: {i+1}')\n",
    "    dataset = get_KDDCUP99(PATH, seed*(i+1), percAnomalies = 0.5, show=False)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=torch.FloatTensor(dataset['x_train']).to(device), \n",
    "                          batch_size = batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "    val_dataset = torch.utils.data.TensorDataset(torch.tensor(dataset['x_val']).to(device), \n",
    "                                              torch.tensor(dataset['y_val'].to_numpy()).to(device))\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.tensor(dataset['x_test']).to(device), \n",
    "                                              torch.tensor(dataset['y_test'].to_numpy()).to(device))\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    trainer = AADNet(device, dataset['selectedColumns'], dataset['discreteCol'], dataset['index'], dataset['x_train'].shape[1])\n",
    "    \n",
    "    d_losses, g_losses, rec_losses, bce_losses, kldes, real_scores, fake_scores, d_losses_val = trainer.train(train_loader, val_loader, batch_size = batch_size, num_epochs = num_epochs)\n",
    "    \n",
    "    plotLoss(d_losses, g_losses, d_losses_val, bce_losses, rec_losses, kldes, real_scores, fake_scores, i, DATASET, show = True)\n",
    "    \n",
    "    prediction, y_true, yP = trainer.predict(test_loader)\n",
    "    auc = roc_auc_score(1-y_true, 1-yP)\n",
    "    print(f'AUC: {auc:.2f}')\n",
    "    auc_list.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc_list)\n",
    "print(f'Mean Auc: {np.mean(auc_list):.2f}, std: {np.std(auc_list):.2f}')\n",
    "\n",
    "GCN_KDDCUP99_Contaminated50 = pd.DataFrame(auc_list, columns = ['GCN_KDDCUP99_Contaminated50'])\n",
    "print(GCN_KDDCUP99_Contaminated50)\n",
    "\n",
    "GCN_KDDCUP99_Contaminated50.to_csv('./GCN_KDDCUP99_Contaminated50.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_KDDCUP99_Contaminated50 = list(GCN_KDDCUP99_Contaminated50['GCN_KDDCUP99_Contaminated50'])\n",
    "\n",
    "N = len(GCN_KDDCUP99_Contaminated50)\n",
    "mean_auc = np.mean(GCN_KDDCUP99_Contaminated50)\n",
    "std_auc = np.std(GCN_KDDCUP99_Contaminated50)\n",
    "std_error = std_auc / (np.sqrt(N))\n",
    "\n",
    "ci = 1.96 * std_error\n",
    "lower_bound = mean_auc - ci\n",
    "upper_bound = mean_auc + ci\n",
    "\n",
    "print(f'{mean_auc:.2f} +/- {ci:.2f}')\n",
    "print(f'95% confidence level, average auc would be between {lower_bound:.2f} and {upper_bound:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
